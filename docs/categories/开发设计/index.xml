<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>开发设计 on Just4Fun</title>
    <link>https://hujianxin.github.io/blog/categories/%E5%BC%80%E5%8F%91%E8%AE%BE%E8%AE%A1/</link>
    <description>Recent content in 开发设计 on Just4Fun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hujianxin.github.io/blog/categories/%E5%BC%80%E5%8F%91%E8%AE%BE%E8%AE%A1/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Fencing与脑裂</title>
      <link>https://hujianxin.github.io/blog/posts/fencing/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hujianxin.github.io/blog/posts/fencing/</guid>
      <description>在使用ZK选主或者作为分布式锁的系统中，可能会出现脑裂问题。 原因可能是：
 节点A的假死，会让ZK认为A是真死，所以将锁让给B。但是A并不认识自己已死，复活之后会继续写目标数据。 ZK某些节点的假死，可能让两个节点都认为自己是Active节点。  所以，ZK解决不了脑裂问题。
HDFS NameNode如何解决脑裂问题 背景： HDFS的NameNode同时只能存在一个Active节点。
HDFS通过JournalNode来解决脑裂问题。
NameNode往JournalNode写edit log的时候，需要带上一个epoch，如果epoch比JournalNode的小，则会被拒绝。
 A节点为Active节点，假设Epoch是7 A节点假死 B节点被选为Active节点，Epoch加一，变成8， B节点写editlog，JournalNode更新epoch为8 A节点复活，通过Epoch 7去写edit log，被拒绝，解决了脑裂问题。  小米云自研消息队列Talos是如何解决脑裂问题 背景：
 Talos需要保证同一条消息必须连续存储 同一个Partition里面的消息必须顺序存储。  所以要求：同一时刻，同一个Partition（HDFS目录）只能有一个Talos Server写入。
为了解决上面问题，Talos使用了ZK作为分布式锁，将TalosServer与Partition进行绑定。但是如最上所说，Zk无法解决脑裂问题：
 如果A节点获取到了PartitionA的锁。 A节点假死，锁TTL超时。 B节点获取到Partition的锁。 A节点复活，继续写这个Partition。  如此就有了脑裂问题。
Talos也是通过Fencing机制来解决脑裂问题。不过与HDFS NameNode形式稍有不同。
解决步骤：
 节点A获取到锁 A假死，锁TTL超时。 B获取到锁，获取这个目录下所有的文件，将最后一个文件关闭，并且新建一个名字为{LastOffset+1}的文件，其中LastOffset为最后一个文件的最后一个消息的Offset。 A复活，继续写文件，发现文件已经关闭，新建{LastOffset+1}的文件，发现已经存在，所以A就会放弃这个锁。  参考  Talos读写一致性 NameNode HA实现原理  </description>
    </item>
    
    <item>
      <title>LSM Tree</title>
      <link>https://hujianxin.github.io/blog/posts/lsm/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hujianxin.github.io/blog/posts/lsm/</guid>
      <description>LSM Tree是HBase使用的数据存储结构。主要原因是，HBase基于HDFS作为底层文件存储，而HDFS不支持随机写，只支持顺序写。而LSM Tree可以轻松应对这种使用场景。
随机写磁盘会导致大量的磁盘寻道，影响写入效率，LSM本身是顺序写入，在磁盘写入方面具有先天优势。另外，有些写多读少的应用场景也非常适合使用LSM Tree。
HBase的LSM Tree实现 在HBase设计中，数据分为两部分：MemStore、DiskStore。
新写入的数据直接写入MemStore，但是为了防止数据丢失，会写一条WAL。当MemStore达到一定的阈值，将其设置只读，并新建一个新的MemStore用来写入，只读的MemStore会被异步的刷成一个DiskStore。
HBase的compaction LSM Tree实现的系统，因为是append only的设计，所以compact是必须的过程。HBase中，分为两种compact：minor compact，major compact
minor compact minor compact是选取部分小的、相邻的HFile，合并成一个大的HFile
major compact major compact，是将整个DiskSotre合并成一个HFile，合并过程中，会删除：过期数据、已经打了删除标记的数据、版本号超额的数据。
分层压缩：LevelDB的LSM Tree实现 LevelDB也是使用了LSM Tree的设计，但是他的compact过程与HBase不同，采用了分层压缩的设计。
如上图所示：
 Level0层，是由ImmutableMem Table dump出来的。所以Level0层不同文件可能会有重叠的。 Level n是由level n-1和level n层compact出来的。 读数据时，顺序是内存-&amp;gt;Level0-&amp;gt;Level1-&amp;gt;Level&amp;hellip;，在查询Level0时，需要查询多个文件，查询其他层时，只需要查询特定文件。  </description>
    </item>
    
    <item>
      <title>ZooKeeper选举</title>
      <link>https://hujianxin.github.io/blog/posts/zk/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://hujianxin.github.io/blog/posts/zk/</guid>
      <description>Zk的快速选举以及一致性保证 概念  myid，每个节点的唯一标识，是存在服务器上特定位置的一个文件，里面有唯一标识的数字 zxid，64位的数字，前32位表示Leader的epoch;后32位表示该epoch内的序号，也就是第几次原子写入。  服务器状态  LOOKING FOLLOWING LEADING OBSERVING  选票结构  logicClock，表示这是该服务器发起的地几轮投票 state self_id self_zxid vote_id vote_zxid  投票过程  先比较logicClock 再比较zxid 再比较myid  Commit过的数据不丢失 快速选举与一致性保证都属于ZAB算法，zk通过快速选举中的zxid，保证在leader宕机之后，选出来的新leader包含所有commit过的数据。
未Commit过的消息对客户端不可见 新Leader会通过zxid判断，将Follower中未Commit，也未过半的消息删除。
如何利用ZK选主（分布式锁原理等同） 概念 zk作为选主工具，主要依赖与这两种概念：
 Persist节点和Ephemeral节点分类，其中Ephemeral节点会在客户端与zk断开session连接时，自动删除。这是选主的必备特性，用于广播leader宕机事件。 Sequence节点和Non-Sequence节点分类，这两者用于决定使用公平选主、非公平选主。 其中，Sequence是公平选主， 因为Leader宕机之后，会按顺序通知下一个节点自动成为Leader。而Non-Sequence是非公平选主，Leader宕机之后，会通知所有节点，进行竞争，重新竞选Leader。  </description>
    </item>
    
    <item>
      <title>在Spring边缘试探</title>
      <link>https://hujianxin.github.io/blog/posts/try-spring/</link>
      <pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://hujianxin.github.io/blog/posts/try-spring/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://cnbj2.fds.api.xiaomi.com/v2tool/3780192716361256736?GalaxyAccessKeyId=5411787938902&amp;amp;Expires=202212332102020&amp;amp;Signature=pbzDTCQBrceHtWVeGqKFwGIrJQk=&#34; alt=&#34;spring-by-pivotal.png&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文首发于简书，于2018年11月迁移至本博客&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>